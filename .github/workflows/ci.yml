# =============================================================================
# CI Workflow — Run the full test matrix
#
# Automatic triggers (with path filters on Rust/test files):
#   push/PR → main : unit (Linux) + integration tests (~15 min gate)
#   push → main    : + E2E tests, + coverage upload
#   Weekly (Mon)   : full unit matrix (Linux + macOS), dbt, CNPG smoke
#   Manual         : all jobs, all platforms (including Windows + benchmarks)
# =============================================================================
name: CI

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'Cargo.toml'
      - 'Cargo.lock'
      - 'tests/**'
      - '.github/workflows/ci.yml'
      - '.github/actions/**'
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'Cargo.toml'
      - 'Cargo.lock'
      - 'tests/**'
      - '.github/workflows/ci.yml'
      - '.github/actions/**'
  schedule:
    - cron: '0 6 * * 1'  # Weekly Monday 06:00 UTC — full matrix
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  PG_VERSION: "18"

jobs:
  # ── Unit tests (cross-platform) ─────────────────────────────────────────
  unit-tests:
    name: Unit tests (${{ matrix.artifact_suffix }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15
    continue-on-error: ${{ matrix.os == 'windows-2022' }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            artifact_suffix: linux-amd64
          - os: macos-14
            artifact_suffix: macos-arm64
          - os: windows-2022
            artifact_suffix: windows-amd64
    steps:
      - uses: actions/checkout@v6

      - name: Setup pgrx environment
        uses: ./.github/actions/setup-pgrx

      - name: Run unit tests (Linux / macOS)
        if: runner.os != 'Windows'
        run: ./scripts/run_unit_tests.sh pg18

      - name: Compile-check unit tests (Windows)
        if: runner.os == 'Windows'
        run: cargo test --lib --features pg18 --no-run

  # ── Integration tests (testcontainers, Linux only) ──────────────────────
  integration-tests:
    name: Integration tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v6

      - name: Setup pgrx environment
        uses: ./.github/actions/setup-pgrx

      - name: Run integration tests
        run: |
          cargo test \
            --test catalog_tests \
            --test extension_tests \
            --test monitoring_tests \
            --test smoke_tests \
            --test resilience_tests \
            --test scenario_tests \
            --test workflow_tests \
            --test property_tests \
            -- --test-threads=1

  # ── E2E tests (full extension in Docker, Linux only) ────────────────────
  # Skipped on PRs (takes ~20 min to build the Docker image). Runs on
  # every push to main so a merge never goes unvalidated end-to-end.
  e2e-tests:
    name: E2E tests
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request'
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v6

      - name: Setup pgrx environment
        uses: ./.github/actions/setup-pgrx

      - name: Build E2E Docker image
        run: ./tests/build_e2e_image.sh

      - name: Run E2E test suite
        run: cargo test --test 'e2e_*' -- --test-threads=1

  # ── Benchmarks (non-blocking, Linux only) ───────────────────────────────
  # Expensive; run on weekly schedule and manual dispatch only.
  benchmarks:
    name: Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 20
    continue-on-error: true
    steps:
      - uses: actions/checkout@v6

      - name: Setup pgrx environment
        uses: ./.github/actions/setup-pgrx

      - name: Run benchmarks
        run: cargo bench --bench refresh_bench --bench diff_operators

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: target/criterion/
          retention-days: 7
          if-no-files-found: ignore

  # ── dbt-pgstream integration tests (Linux only) ──────────────────────────
  # Cost-optimized: single job builds the Docker image once, then tests
  # against dbt-core min (1.6) and max (1.9). Intermediate versions are
  # skipped — if both boundary versions pass, the middle ones will too.
  # Runs on weekly schedule and manual dispatch only (slow Docker build).
  dbt-integration:
    name: dbt integration
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v6

      - name: Build E2E Docker image
        run: ./tests/build_e2e_image.sh

      - name: Start PostgreSQL with pg_stream
        run: |
          docker run -d \
            --name pgstream-dbt-test \
            -e POSTGRES_PASSWORD=postgres \
            -p 5432:5432 \
            pg_stream_e2e:latest

          # Wait for PostgreSQL to accept connections
          echo "Waiting for PostgreSQL to be ready..."
          for i in $(seq 1 30); do
            if docker exec pgstream-dbt-test \
              pg_isready -U postgres -q 2>/dev/null; then
              echo "PostgreSQL is ready (attempt $i)"
              break
            fi
            sleep 1
          done

      - name: Create pg_stream extension
        run: |
          docker exec pgstream-dbt-test \
            psql -U postgres -c "CREATE EXTENSION pg_stream;"

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Test with dbt 1.6 (minimum supported)
        env:
          PGHOST: localhost
          PGPORT: '5432'
          PGUSER: postgres
          PGPASSWORD: postgres
          PGDATABASE: postgres
        run: |
          pip install "dbt-core~=1.6.0" "dbt-postgres~=1.6.0"
          echo "=== dbt version ==="
          dbt --version
          cd dbt-pgstream/integration_tests
          dbt deps
          dbt seed
          dbt run
          ./scripts/wait_for_populated.sh order_totals 30
          dbt test
          dbt run --full-refresh
          ./scripts/wait_for_populated.sh order_totals 30
          dbt test
          dbt run-operation pgstream_refresh --args '{model_name: order_totals}'
          dbt run-operation pgstream_check_freshness
          dbt run-operation drop_all_stream_tables

      - name: Test with dbt 1.9 (latest supported)
        env:
          PGHOST: localhost
          PGPORT: '5432'
          PGUSER: postgres
          PGPASSWORD: postgres
          PGDATABASE: postgres
        run: |
          pip install "dbt-core~=1.9.0" "dbt-postgres~=1.9.0"
          echo "=== dbt version ==="
          dbt --version
          cd dbt-pgstream/integration_tests
          dbt deps
          dbt seed
          dbt run
          ./scripts/wait_for_populated.sh order_totals 30
          dbt test
          dbt run --full-refresh
          ./scripts/wait_for_populated.sh order_totals 30
          dbt test
          dbt run-operation pgstream_refresh --args '{model_name: order_totals}'
          dbt run-operation pgstream_check_freshness
          dbt run-operation drop_all_stream_tables

      - name: Cleanup
        if: always()
        run: docker rm -f pgstream-dbt-test 2>/dev/null || true

  # ── CloudNativePG smoke test (non-blocking, Linux only) ─────────────────
  # Runs on weekly schedule and manual dispatch only (requires kind cluster).
  #
  # Uses a transitional approach: builds the scratch-based extension image
  # (cnpg/Dockerfile.ext-build), then creates a composite image combining
  # the extension with postgres:18.1 for the smoke test. This validates
  # that the extension image layout is correct and the extension works.
  #
  # Once kind supports Kubernetes 1.33 with ImageVolume feature gate, this
  # should be updated to use .spec.postgresql.extensions directly with the
  # official CNPG operand image.
  cnpg-smoke-test:
    name: CNPG smoke test
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 20
    continue-on-error: true
    steps:
      - uses: actions/checkout@v6

      - name: Create kind cluster
        uses: helm/kind-action@v1
        with:
          cluster_name: cnpg-test

      - name: Build extension image (scratch-based)
        run: |
          docker build -t pg_stream-ext:ci \
            -f cnpg/Dockerfile.ext-build .

      - name: Verify extension image layout
        run: |
          ID=$(docker create pg_stream-ext:ci)
          mkdir -p /tmp/ext-verify
          docker cp "$ID:/lib/" /tmp/ext-verify/lib/
          docker cp "$ID:/share/" /tmp/ext-verify/share/
          docker rm "$ID"

          echo "=== Extension image contents ==="
          find /tmp/ext-verify -type f
          echo "==============================="

          test -f /tmp/ext-verify/lib/pg_stream.so
          test -f /tmp/ext-verify/share/extension/pg_stream.control
          ls /tmp/ext-verify/share/extension/pg_stream--*.sql

          echo "Extension image layout OK"

      - name: Build composite image for smoke test
        run: |
          # Transitional: combine extension image with postgres:18.1
          # until kind supports K8s 1.33 ImageVolume feature gate.
          # This image is NOT shipped — only used for CI validation.
          cat > /tmp/Dockerfile.cnpg-smoke <<'DOCKERFILE'
          FROM pg_stream-ext:ci AS ext
          FROM postgres:18.1
          COPY --from=ext /lib/ /usr/lib/postgresql/18/lib/
          COPY --from=ext /share/extension/ /usr/share/postgresql/18/extension/
          DOCKERFILE

          docker build -t pg_stream:18.1 -f /tmp/Dockerfile.cnpg-smoke .

      - name: Load image into kind
        run: kind load docker-image pg_stream:18.1 --name cnpg-test

      - name: Install CNPG operator
        run: |
          kubectl apply --server-side -f \
            https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.0.yaml
          kubectl wait --for=condition=Available \
            deployment/cnpg-controller-manager \
            -n cnpg-system \
            --timeout=120s

      - name: Deploy cluster with extension
        run: |
          # Use an inline CI-specific manifest.
          # imagePullPolicy: Never is required for kind-loaded images.
          #
          # Do NOT put shared_preload_libraries here: CNPG propagates
          # postgresql.conf (incl. shared_preload_libraries) into the
          # initdb container's temporary PostgreSQL start. If _PG_init()
          # has any issue in that early phase the initdb pod crashes.
          # Our _PG_init() already handles the missing-SPL case: it
          # registers GUCs but skips shmem/bgworker — all DDL/DML still
          # works. Background scheduler is not needed for a smoke test.
          kubectl apply -f - <<'EOF'
          apiVersion: postgresql.cnpg.io/v1
          kind: Cluster
          metadata:
            name: pg-stream-demo
          spec:
            instances: 1
            imageName: pg_stream:18.1
            imagePullPolicy: Never
            bootstrap:
              initdb:
                database: app
                owner: app
            storage:
              size: 1Gi
              storageClass: standard
          EOF

      - name: Wait for cluster primary pod
        run: |
          echo "Waiting for CNPG Cluster to be ready (up to 8 min)..."
          kubectl wait cluster/pg-stream-demo \
            --for=condition=Ready \
            --timeout=480s || {
              echo "--- Cluster status ---"
              kubectl describe cluster/pg-stream-demo | tail -40
              echo "--- Pod status ---"
              kubectl get pods -l cnpg.io/cluster=pg-stream-demo -o wide
              echo "--- Initdb pod logs (most recent) ---"
              FAILED_POD=$(kubectl get pods \
                -l cnpg.io/cluster=pg-stream-demo \
                --field-selector=status.phase=Failed \
                -o jsonpath='{.items[-1].metadata.name}' 2>/dev/null || true)
              if [[ -n "$FAILED_POD" ]]; then
                echo "Logs for ${FAILED_POD}:"
                kubectl logs "${FAILED_POD}" --all-containers 2>&1 | tail -60
              fi
              exit 1
            }

          echo "Waiting for primary pod to be Ready (up to 3 min)..."
          kubectl wait pods \
            -l cnpg.io/cluster=pg-stream-demo,role=primary \
            --for=condition=Ready \
            --timeout=180s

      - name: Run CNPG smoke test
        run: |
          PRIMARY=$(kubectl get pods \
            -l cnpg.io/cluster=pg-stream-demo \
            -l role=primary \
            -o jsonpath='{.items[0].metadata.name}')

          if [[ -z "$PRIMARY" ]]; then
            echo "ERROR: no primary pod found"
            kubectl get pods -l cnpg.io/cluster=pg-stream-demo
            exit 1
          fi

          echo "Primary pod: ${PRIMARY}"

          # Run all checks in a SINGLE psql session.
          #
          # Why one session? Without shared_preload_libraries the extension
          # .so is loaded dynamically when CREATE EXTENSION runs. PostgreSQL
          # only registers GUCs (SHOW pg_stream.*) for the lifetime of the
          # connection that triggered the dynamic load. Separate kubectl exec
          # calls each start a new backend that hasn't loaded the library yet,
          # so SHOW pg_stream.enabled returns "unrecognized configuration parameter".
          # Consolidating into one session avoids this limitation entirely.
          #
          # Note: heredoc (<<EOF) is avoided here because GitHub Actions YAML
          # parses << as a merge key even inside run: | block scalars,
          # causing a syntax error. Use -c with a multi-line string instead.
          kubectl exec "${PRIMARY}" -- \
            psql -U postgres -d app -v ON_ERROR_STOP=1 -c "
              CREATE EXTENSION pg_stream;
              SELECT extname, extversion FROM pg_extension WHERE extname = 'pg_stream';
              SHOW pg_stream.enabled;
              CREATE TABLE cnpg_test_src (id INT PRIMARY KEY, val TEXT);
              INSERT INTO cnpg_test_src VALUES (1, 'hello');
              SELECT pgstream.create_stream_table(
                  'cnpg_test_dt',
                  'SELECT id, val FROM cnpg_test_src',
                  '1m',
                  'FULL'
              );
              SELECT * FROM cnpg_test_dt;
              SELECT pgstream.drop_stream_table('cnpg_test_dt');
              DROP TABLE cnpg_test_src;
            "

          echo "CNPG smoke test passed!"
